################rapport#################

########################################
NETWORK_CONFIGURATION 
nano /etc/netplan/00-installer-config.yaml
sudo netplan apply
# This is the network config written by 'subiquity'
network:
  ethernets:
    ens33:
      dhcp4: false
      addresses: [192.168.43.44/24]
      gateway4: 192.168.43.1
      nameservers:
        addresses: [8.8.8.8, 8.8.4.4]
    ens38:
      dhcp4: true
  version: 2

#############################TWO_INSTANCES#####################################
# Provisionning OpenStack instances using Terraform
terraform {
  required_version = ">= 0.14.0"
  required_providers {
    openstack = {
      source  = "terraform-provider-openstack/openstack"
      version = "1.49.0"
    }
  }
}

# Defining the provider
provider "openstack" {
  user_name   = "admin"
  password    = "x"
  tenant_name = "admin"
  auth_url    = "http://192.168.10.208:5000/v3"
  region      = "RegionOne"
}

# Defining the variables
variable "instance_name" {
  default = "Node"
}

variable "boot_volume_id" {
  default = ["7bae7b61-601b-427c-a932-f06d4fb0b157", "5596047f-0f91-48f4-bd20-47da0406c219"]
}

variable "network_name" {
  default = "selfservice"
}

# Creating the instances with the existing bootable volumes
resource "openstack_compute_instance_v2" "k8s_instance" {
  count           = length(var.boot_volume_id)
  name            = "${var.instance_name}-${count.index}"
  flavor_name     = "m2.huh"
  key_pair        = "k8s_key"
  security_groups = ["default"]

  block_device {
    uuid             = var.boot_volume_id[count.index]
    source_type      = "volume"
    destination_type = "volume"
    boot_index       = 0
  }

  network {
    name = var.network_name
  }

  # Specify cloud-init configuration
  user_data = <<-EOF
    #cloud-config
    password: x
    chpasswd: { expire: False }
    ssh_pwauth: True
  EOF
}

# Create floating IPs
resource "openstack_networking_floatingip_v2" "floating_ip" {
  count = length(openstack_compute_instance_v2.k8s_instance)
  pool  = "provider"
}

# Associate floating IPs with the instances
resource "openstack_compute_floatingip_associate_v2" "associate_floating_ip" {
  count       = length(openstack_compute_instance_v2.k8s_instance)
  floating_ip = openstack_networking_floatingip_v2.floating_ip[count.index].address
  instance_id = openstack_compute_instance_v2.k8s_instance[count.index].id
  depends_on  = [openstack_compute_instance_v2.k8s_instance]
}


########################################
kubeadm installation : 
https://www.youtube.com/watch?v=xX52dc3u2HU
https://github.com/techiescamp/kubeadm-scripts
https://devopscube.com/setup-kubernetes-cluster-kubeadm/


########################################
experiment_replicaset : 
https://www.youtube.com/watch?v=y_vy9NVeCzo


########################################
fix_bugs CRI-O_Kubeadm :
rm -rf /etc/apt/sources.list.d/
mkdir -p /etc/apt/sources.list.d/
mkdir -p  /etc/apt/keyrings/
https://www.linuxtechi.com/install-crio-container-runtime-on-u 

########################################
fix_bugs Containerd :
su root  
apt remove containerd
apt update
apt install containerd.io
rm /etc/containerd/config.toml
systemctl restart containerd

########################################

fix_Kubernetes â€œx509: certificate has expired : 
kubeadm certs renew all
sudo cp -i /etc/kubernetes/admin.conf "$HOME"/.kube/config
systemctl restart kubelet 

########################################
KUBEADM CONFIG : 
kubectl get po -n kube-system
kubectl get --raw='/readyz?verbose'
kubeadm token create --print-join-command
kubectl label node worker node-role.kubernetes.io/worker=worker


########################################
Install Helm  :

 https://phoenixnap.com/kb/install-helm

#################UU6GXEUJ#######################
----MetalLB Loadbalancer : 

Apply MetalLB manifests : 
kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.12.1/manifests/namespace.yaml
kubectl apply -f https://raw.githubusercontent.com/metallb/metallb/v0.12.1/manifests/metallb.yaml

Configure MetalLB with a pool of IP addresses : 
metallb-configmap.yaml

apiVersion: v1
kind: ConfigMap
metadata:
  namespace: metallb-system
  name: config
data:
  config: |
    address-pools:
    - name: default
      protocol: layer2
      addresses:
      - 192.168.10.100-192.168.10.130

Apply the ConfigMap :
kubectl apply -f metallb-configmap.yaml

Verify MetalLB Installation :
kubectl get pods -n metallb-system


########################################
----Setting Up Prometheus in Kubernetes :

Step 1: Install Prometheus on the Kubernetes Master Node : 

helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo add grafana https://grafana.github.io/helm-charts
helm repo update


Install the Prometheus stack:
helm install prometheus prometheus-community/kube-prometheus-stack --namespace monitoring --create-namespace

Enable Grafana

helm show values prometheus-community/kube-prometheus-stack > values.yaml

grafana:
  enabled: true
  adminPassword: "your-admin-password" 
  persistence:
    enabled: true
    size: 10Gi
  service:
    type: LoadBalancer
    port: 3000
    portName: http-web
    ipFamilies: []
    ipFamilyPolicy: ""


2. Upgrade the Helm Release
helm upgrade --install prometheus prometheus-community/kube-prometheus-stack --namespace monitoring --create-namespace -f values.yaml

3. Verify the Deployment
kubectl get pods -n monitoring

########################################
########################################
########################################
############### D_E_V_O_P_S ############
########################################
########################################
########################################

########################################
install npm : 

curl -o- https://raw.githubusercontent.com/nvm-sh/nvm/v0.39.1/install.sh | bash
source ~/.bashrc
nvm install 18.20.2
node -v

########################################
Installing helm :
wget https://get.helm.sh/helm-v3.9.3-linux-amd64.tar.gz
tar xvf helm-v3.9.3-linux-amd64.tar.gz
sudo mv linux-amd64/helm /usr/local/bin
rm helm-v3.9.3-linux-amd64.tar.gz
helm version
########################################
ARgoCD:
kubectl create namespace argocd
kubectl apply -n argocd -f https://raw.githubusercontent.com/argoproj/argo-cd/stable/manifests/install.yaml
kubectl get pods -n argocd
kubectl patch svc argocd-server -n argocd -p '{"spec": {"type": "LoadBalancer"}}'
kubectl get svc argocd-server -n argocd
kubectl get secret argocd-initial-admin-secret -n argocd -o yaml
echo "password" | base64 --decode


Username: admin
Password: GyzdZUGA5lCAqmBU

########################################
Installing GitLab Runner :
values.yaml : 

gitlabUrl: https://gitlab.com/
runnerRegistrationToken: "glrt-w4iRi7vu-9oiPh954aVH"
rbac:
  create: true
runners:
  config: |
    [[runners]]
      [runners.kubernetes]
        namespace = "gitlab-runner"
  privileged: true
  namespace: "gitlab-runner"
  serviceAccountName: "gitlab-runner"
  image: "alpine:latest"
  resources:
    requests:
      memory: "256Mi"
      cpu: "100m"
    limits:
      memory: "512Mi"
      cpu: "200m"
  environment:
  - FF_USE_LEGACY_KUBERNETES_EXECUTION_STRATEGY=true

  
 helm repo add gitlab https://charts.gitlab.io
 helm repo update
 helm install gitlab-runner gitlab/gitlab-runner --namespace gitlab-runner --create-namespace -f values.yaml
 #upgrade : 
 helm upgrade gitlab-runner gitlab/gitlab-runner --namespace gitlab-runner  --values values.yaml 


kubectl rollout restart deployment gitlab-runner -n gitlab-runner


helm upgrade gitlab-runner \
    --namespace gitlab-runner \
    --values values.yaml \
    --set gitlabUrl=https://gitlab.com,runnerRegistrationToken=glrt-w4iRi7vu-9oiPh954aVH \
    gitlab/gitlab-runner


    iXuBsQbuhLYjVAlF

#########################################
metrics error : 
    livenessProbe:
  httpGet:
    path: /
    port: http
  initialDelaySeconds: 300       
  periodSeconds: 20
  timeoutSeconds: 10
After which the liveness probe started executing successfully

Same can be done for the readiness probe:

ReadinessProbe:
  httpGet:
    path: /
    port: http
  initialDelaySeconds: 30
  periodSeconds: 20
  timeoutSeconds: 10


